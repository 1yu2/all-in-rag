"""
å¼‚æ­¥ VLM è§†è§‰è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
========================

æä¾›å¼‚æ­¥è°ƒç”¨è§†è§‰è¯­è¨€æ¨¡å‹çš„ç®€åŒ–æ¥å£ï¼Œæ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨ã€‚

åŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼š
============

1. éæµå¼è°ƒç”¨ï¼š
   from app.llm.vlm_client import call_vlm_async
   response = await call_vlm_async("æè¿°è¿™å¼ å›¾ç‰‡", images=["path/to/image.jpg"])

2. æµå¼è°ƒç”¨ï¼š
   from app.llm.vlm_client import call_vlm_stream_async
   async for chunk in call_vlm_stream_async("åˆ†æå›¾ç‰‡å†…å®¹", images=["image.jpg"]):
       print(chunk, end='')

3. è‡ªå®šä¹‰å‚æ•°ï¼š
   response = await call_vlm_async(
       "æè¿°å›¾ç‰‡",
       images=["image.jpg"],
       temperature=0.5,     # æ§åˆ¶éšæœºæ€§
       max_tokens=1024      # æœ€å¤§è¾“å‡ºé•¿åº¦
   )

å›¾ç‰‡æ”¯æŒï¼š
=========
æ”¯æŒå¤šç§å›¾ç‰‡è¾“å…¥æ ¼å¼ï¼š
- æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²: "path/to/image.jpg"
- å­—èŠ‚æ•°æ®: bytes
"""
import asyncio
import base64
import json
import logging
from typing import AsyncGenerator, Dict, List, Union
import requests
import sys
import os

logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# å¯¼å…¥é…ç½®
try:
    from code.config import settings
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå°è¯•å…¶ä»–è·¯å¾„
    import importlib.util
    config_path = os.path.join(project_root, 'config', 'config.py')
    if os.path.exists(config_path):
        spec = importlib.util.spec_from_file_location("config_module", config_path)
        config_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(config_module)
        settings = config_module.settings
    else:
        raise ImportError(f"Cannot find config file at {config_path}")


def _get_vlm_config() -> dict:
    """è·å– VLM æ¨¡å‹é…ç½®"""
    return {
        "api_url": settings.VL_API_URL,
        "api_key": settings.VL_API_KEY,
        "model_name": settings.VL_MODEL_NAME,
        "temperature": settings.VL_TEMPERATURE,
        "timeout": settings.VL_TIMEOUT
    }


async def _encode_image_to_base64(image_input: Union[str, bytes]) -> str:
    """å¼‚æ­¥å°†å›¾ç‰‡ç¼–ç ä¸º base64 å­—ç¬¦ä¸²"""
    try:
        if isinstance(image_input, str):
            # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œå¼‚æ­¥è¯»å–æ–‡ä»¶
            loop = asyncio.get_event_loop()
            with open(image_input, "rb") as f:
                image_data = await loop.run_in_executor(None, f.read)
        elif isinstance(image_input, bytes):
            image_data = image_input
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å›¾ç‰‡ç±»å‹: {type(image_input)}ï¼Œä»…æ”¯æŒæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®")

        return base64.b64encode(image_data).decode('utf-8')
    except Exception as e:
        logger.error(f"å›¾ç‰‡ç¼–ç å¤±è´¥: {e}")
        raise


async def _make_request_payload(prompt: str, images: List[Union[str, bytes]],
                                stream: bool = False, **kwargs) -> Dict:
    """æ„å»ºè¯·æ±‚è´Ÿè½½"""
    config = _get_vlm_config()

    # å¤„ç†å›¾ç‰‡
    content = [{"type": "text", "text": prompt}]

    for img in images:
        img_b64 = await _encode_image_to_base64(img)
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}
        })

    payload = {
        "model": config["model_name"],
        "messages": [{"role": "user", "content": content}],
        "stream": stream,
        "temperature": kwargs.get("temperature", config["temperature"]),
    }

    # åªæœ‰åœ¨æä¾›äº†max_tokenså‚æ•°æ—¶æ‰æ·»åŠ 
    if "max_tokens" in kwargs:
        payload["max_tokens"] = kwargs["max_tokens"]

    return payload


def _extract_content(data: Dict) -> str:
    """æå–å“åº”å†…å®¹"""
    if 'choices' in data and data['choices']:
        choice = data['choices'][0]
        if 'message' in choice:
            return choice['message'].get('content', '')
        elif 'delta' in choice:
            return choice['delta'].get('content', '')
    return ''


async def call_vlm_async(prompt: str, images: List[Union[str, bytes]], **kwargs) -> str:
    """å¼‚æ­¥è°ƒç”¨ VLM è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆéæµå¼ï¼‰

    Args:
        prompt: è¾“å…¥æç¤ºè¯
        images: å›¾ç‰‡åˆ—è¡¨ï¼ˆæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®ï¼‰
        **kwargs: å¯é€‰å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

    Returns:
        str: æ¨¡å‹å›å¤å†…å®¹

    Raises:
        ValueError: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
    """
    if not images:
        raise ValueError("VLMè°ƒç”¨å¿…é¡»æä¾›è‡³å°‘ä¸€å¼ å›¾ç‰‡")

    config = _get_vlm_config()
    payload = await _make_request_payload(prompt, images, stream=False, **kwargs)

    headers = {
        "Content-Type": "application/json",
        "szc-api-key": config["api_key"]
    }

    try:
        # ä½¿ç”¨ asyncio åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥è¯·æ±‚
        loop = asyncio.get_event_loop()

        def _sync_request():
            response = requests.post(
                config["api_url"],
                json=payload,
                headers=headers,
                timeout=config["timeout"]
            )
            response.raise_for_status()
            return response.json()

        result = await loop.run_in_executor(None, _sync_request)
        content = _extract_content(result)
        if not content:
            raise ValueError("APIè¿”å›ç©ºå†…å®¹")

        return content

    except requests.RequestException as e:
        logger.error(f"VLM APIè°ƒç”¨å¤±è´¥: {e}")
        raise ValueError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"VLMè°ƒç”¨å¼‚å¸¸: {e}")
        raise ValueError(f"APIè°ƒç”¨å¤±è´¥: {e}")


async def call_vlm_stream_async(prompt: str, images: List[Union[str, bytes]], **kwargs) -> AsyncGenerator[str, None]:
    """å¼‚æ­¥è°ƒç”¨ VLM è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆæµå¼ï¼‰

    Args:
        prompt: è¾“å…¥æç¤ºè¯
        images: å›¾ç‰‡åˆ—è¡¨ï¼ˆæ–‡ä»¶è·¯å¾„æˆ–å­—èŠ‚æ•°æ®ï¼‰
        **kwargs: å¯é€‰å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

    Yields:
        str: æ¨¡å‹å›å¤çš„æ–‡æœ¬å—

    Raises:
        ValueError: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
    """
    if not images:
        raise ValueError("VLMè°ƒç”¨å¿…é¡»æä¾›è‡³å°‘ä¸€å¼ å›¾ç‰‡")

    config = _get_vlm_config()
    payload = await _make_request_payload(prompt, images, stream=True, **kwargs)

    headers = {
        "Content-Type": "application/json",
        "szc-api-key": config["api_key"]
    }

    try:
        # ä½¿ç”¨é˜Ÿåˆ—åœ¨çº¿ç¨‹å’Œåç¨‹ä¹‹é—´ä¼ é€’æ•°æ®
        import queue
        import threading

        chunk_queue = queue.Queue()
        exception_holder = [None]

        def _sync_stream_request():
            try:
                response = requests.post(
                    config["api_url"],
                    json=payload,
                    headers=headers,
                    timeout=config["timeout"],
                    stream=True
                )
                response.raise_for_status()

                for line in response.iter_lines():
                    if not line:
                        continue

                    line_str = line.decode("utf-8").strip()
                    if not line_str.startswith("data:"):
                        continue

                    chunk_data = line_str[5:].strip()
                    if chunk_data == "[DONE]":
                        break

                    try:
                        data = json.loads(chunk_data)
                        content = _extract_content(data)
                        if content:
                            chunk_queue.put(content)
                    except json.JSONDecodeError:
                        continue

                chunk_queue.put(None)  # ç»“æŸæ ‡è®°
            except Exception as e:
                exception_holder[0] = e
                chunk_queue.put(None)

        # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥æµå¼è¯·æ±‚
        thread = threading.Thread(target=_sync_stream_request)
        thread.start()

        # å¼‚æ­¥è·å–æ•°æ®
        while True:
            # éé˜»å¡æ£€æŸ¥é˜Ÿåˆ—
            try:
                chunk = chunk_queue.get_nowait()
                if chunk is None:  # ç»“æŸæ ‡è®°
                    break
                yield chunk
            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œè®©å‡ºæ§åˆ¶æƒ
                await asyncio.sleep(0.01)
                continue

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        thread.join()

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸
        if exception_holder[0]:
            raise exception_holder[0]

    except requests.RequestException as e:
        logger.error(f"VLMæµå¼APIè°ƒç”¨å¤±è´¥: {e}")
        raise ValueError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"VLMæµå¼è°ƒç”¨å¼‚å¸¸: {e}")
        raise ValueError(f"æµå¼APIè°ƒç”¨å¤±è´¥: {e}")


async def call_vlm_batch_async(prompt_image_pairs: List[tuple], **kwargs) -> List[str]:
    """æ‰¹é‡å¼‚æ­¥è°ƒç”¨ VLM è§†è§‰è¯­è¨€æ¨¡å‹

    Args:
        prompt_image_pairs: [(prompt1, [images1]), (prompt2, [images2]), ...]
        **kwargs: å¯é€‰å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

    Returns:
        List[str]: æ¨¡å‹å›å¤åˆ—è¡¨
    """
    tasks = [call_vlm_async(prompt, images, **kwargs)
             for prompt, images in prompt_image_pairs]
    return await asyncio.gather(*tasks)


def get_vlm_config() -> dict:
    """è·å– VLM é…ç½®ä¿¡æ¯

    Returns:
        dict: é…ç½®ä¿¡æ¯
    """
    return _get_vlm_config()


# ç¤ºä¾‹ç”¨æ³•
async def main():
    """æµ‹è¯•VLMæ¨¡å‹æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æµ‹è¯•VLMæ¨¡å‹è¿æ¥çŠ¶æ€...")

    try:
        config = _get_vlm_config()

        # ç›´æ¥æ„å»ºç®€å•è¯·æ±‚
        payload = {
            "model": config["model_name"],
            "messages": [{"role": "user", "content": "æµ‹è¯•è¿æ¥"}],
            "stream": False,
            "temperature": 0.7
        }

        headers = {
            "Content-Type": "application/json",
            "szc-api-key": config["api_key"]
        }

        # ç›´æ¥å‘é€POSTè¯·æ±‚
        loop = asyncio.get_event_loop()
        def _sync_request():
            response = requests.post(
                config["api_url"],
                json=payload,
                headers=headers,
                timeout=10
            )
            response.raise_for_status()
            return response.json()

        result = await loop.run_in_executor(None, _sync_request)

        # æ£€æŸ¥å“åº”
        if result and 'choices' in result and result['choices']:
            print("âœ… VLMæ¨¡å‹å¯ç”¨")
            return True
        else:
            print("âŒ VLMæ¨¡å‹å“åº”å¼‚å¸¸")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def run_tests():
    """è¿è¡ŒVLMæ¨¡å‹å¯ç”¨æ€§æµ‹è¯•"""
    try:
        result = asyncio.run(main())
        return 0 if result else 1
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        return 1


if __name__ == "__main__":
    exit(run_tests())
