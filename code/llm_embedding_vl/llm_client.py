"""
å¼‚æ­¥ LLM æ–‡æœ¬æ¨¡å‹å®¢æˆ·ç«¯
===================

æä¾›å¼‚æ­¥è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹çš„ç®€åŒ–æ¥å£ï¼Œæ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨ã€‚

åŸºæœ¬ä½¿ç”¨æ–¹æ³•ï¼š
============

1. éæµå¼è°ƒç”¨ï¼š
   from app.llm.llm_client import call_llm_async
   response = await call_llm_async("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹æ·±åœ³")

2. æµå¼è°ƒç”¨ï¼š
   from app.llm.llm_client import call_llm_stream_async
   async for chunk in call_llm_stream_async("å†™ä¸€ç¯‡å…³äºæ·±åœ³çš„æ–‡ç« "):
       print(chunk, end='')

3. è‡ªå®šä¹‰å‚æ•°ï¼š
   response = await call_llm_async(
       "ä½ å¥½",
       temperature=0.5,     # æ§åˆ¶éšæœºæ€§
       max_tokens=1024      # æœ€å¤§è¾“å‡ºé•¿åº¦
   )
"""
import asyncio
import json
import logging
from typing import AsyncGenerator, Dict, List
import requests
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

logger = logging.getLogger(__name__)

# å¯¼å…¥é…ç½®
try:
    from config.config import settings
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå°è¯•å…¶ä»–è·¯å¾„
    import importlib.util
    config_path = os.path.join(project_root, 'config', 'config.py')
    if os.path.exists(config_path):
        spec = importlib.util.spec_from_file_location("config_module", config_path)
        config_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(config_module)
        settings = config_module.settings
    else:
        raise ImportError(f"Cannot find config file at {config_path}")


def _get_llm_config() -> dict:
    """è·å– LLM æ¨¡å‹é…ç½®"""
    return {
        "api_url": settings.LLM_API_URL,
        "api_key": settings.LLM_API_KEY,
        "model_name": settings.LLM_MODEL_NAME,
        "temperature": settings.LLM_TEMPERATURE,
        "timeout": settings.LLM_TIMEOUT
    }


def _make_request_payload(prompt: str, stream: bool = False, **kwargs) -> Dict:
    """æ„å»ºè¯·æ±‚è´Ÿè½½"""
    config = _get_llm_config()

    return {
        "model": config["model_name"],
        "messages": [{"role": "user", "content": prompt}],
        "stream": stream,
        "temperature": kwargs.get("temperature", config["temperature"]),
    }


def _extract_content(data: Dict) -> str:
    """æå–å“åº”å†…å®¹"""
    if 'choices' in data and data['choices']:
        choice = data['choices'][0]
        if 'message' in choice:
            return choice['message'].get('content', '')
        elif 'delta' in choice:
            return choice['delta'].get('content', '')
    return ''


async def call_llm_async(prompt: str, **kwargs) -> str:
    """å¼‚æ­¥è°ƒç”¨ LLM æ–‡æœ¬æ¨¡å‹ï¼ˆéæµå¼ï¼‰

    Args:
        prompt: è¾“å…¥æç¤ºè¯
        **kwargs: å¯é€‰å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

    Returns:
        str: æ¨¡å‹å›å¤å†…å®¹

    Raises:
        ValueError: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
    """
    config = _get_llm_config()
    payload = _make_request_payload(prompt, stream=False, **kwargs)

    headers = {
        "Content-Type": "application/json",
        "szc-api-key": config["api_key"]
    }

    try:
        # ä½¿ç”¨ asyncio åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥è¯·æ±‚
        loop = asyncio.get_event_loop()

        def _sync_request():
            response = requests.post(
                config["api_url"],
                json=payload,
                headers=headers,
                timeout=config["timeout"]
            )
            response.raise_for_status()
            return response.json()

        result = await loop.run_in_executor(None, _sync_request)
        content = _extract_content(result)
        if not content:
            raise ValueError("APIè¿”å›ç©ºå†…å®¹")

        return content

    except requests.RequestException as e:
        logger.error(f"LLM APIè°ƒç”¨å¤±è´¥: {e}")
        raise ValueError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¼‚å¸¸: {e}")
        raise ValueError(f"APIè°ƒç”¨å¤±è´¥: {e}")


async def call_llm_stream_async(prompt: str, **kwargs) -> AsyncGenerator[str, None]:
    """å¼‚æ­¥è°ƒç”¨ LLM æ–‡æœ¬æ¨¡å‹ï¼ˆæµå¼ï¼‰

    Args:
        prompt: è¾“å…¥æç¤ºè¯
        **kwargs: å¯é€‰å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

    Yields:
        str: æ¨¡å‹å›å¤çš„æ–‡æœ¬å—

    Raises:
        ValueError: APIè°ƒç”¨å¤±è´¥æ—¶æŠ›å‡º
    """
    config = _get_llm_config()
    payload = _make_request_payload(prompt, stream=True, **kwargs)

    headers = {
        "Content-Type": "application/json",
        "szc-api-key": config["api_key"]
    }

    try:
        # ä½¿ç”¨é˜Ÿåˆ—åœ¨çº¿ç¨‹å’Œåç¨‹ä¹‹é—´ä¼ é€’æ•°æ®
        import queue
        import threading

        chunk_queue = queue.Queue()
        exception_holder = [None]

        def _sync_stream_request():
            try:
                response = requests.post(
                    config["api_url"],
                    json=payload,
                    headers=headers,
                    timeout=config["timeout"],
                    stream=True
                )
                response.raise_for_status()

                for line in response.iter_lines():
                    if not line:
                        continue

                    line_str = line.decode("utf-8").strip()
                    if not line_str.startswith("data:"):
                        continue

                    chunk_data = line_str[5:].strip()
                    if chunk_data == "[DONE]":
                        break

                    try:
                        data = json.loads(chunk_data)
                        content = _extract_content(data)
                        if content:
                            chunk_queue.put(content)
                    except json.JSONDecodeError:
                        continue

                chunk_queue.put(None)  # ç»“æŸæ ‡è®°
            except Exception as e:
                exception_holder[0] = e
                chunk_queue.put(None)

        # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥æµå¼è¯·æ±‚
        thread = threading.Thread(target=_sync_stream_request)
        thread.start()

        # å¼‚æ­¥è·å–æ•°æ®
        while True:
            # éé˜»å¡æ£€æŸ¥é˜Ÿåˆ—
            try:
                chunk = chunk_queue.get_nowait()
                if chunk is None:  # ç»“æŸæ ‡è®°
                    break
                yield chunk
            except queue.Empty:
                # é˜Ÿåˆ—ä¸ºç©ºï¼Œè®©å‡ºæ§åˆ¶æƒ
                await asyncio.sleep(0.01)
                continue

        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        thread.join()

        # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸
        if exception_holder[0]:
            raise exception_holder[0]

    except requests.RequestException as e:
        logger.error(f"LLMæµå¼APIè°ƒç”¨å¤±è´¥: {e}")
        raise ValueError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
    except Exception as e:
        logger.error(f"LLMæµå¼è°ƒç”¨å¼‚å¸¸: {e}")
        raise ValueError(f"æµå¼APIè°ƒç”¨å¤±è´¥: {e}")


async def call_llm_batch_async(prompts: List[str], **kwargs) -> List[str]:
    """æ‰¹é‡å¼‚æ­¥è°ƒç”¨ LLM æ–‡æœ¬æ¨¡å‹

    Args:
        prompts: æç¤ºè¯åˆ—è¡¨
        **kwargs: å¯é€‰å‚æ•°ï¼ˆtemperature, max_tokens ç­‰ï¼‰

    Returns:
        List[str]: æ¨¡å‹å›å¤åˆ—è¡¨
    """
    tasks = [call_llm_async(prompt, **kwargs) for prompt in prompts]
    return await asyncio.gather(*tasks)


async def test_llm_connection() -> bool:
    """æµ‹è¯• LLM è¿æ¥æ˜¯å¦æ­£å¸¸

    Returns:
        bool: è¿æ¥æ˜¯å¦æ­£å¸¸
    """
    try:
        response = await call_llm_async("æµ‹è¯•è¿æ¥")
        return len(response) > 0
    except Exception as e:
        logger.error(f"LLMè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
        return False


def get_llm_config() -> dict:
    """è·å– LLM é…ç½®ä¿¡æ¯

    Returns:
        dict: é…ç½®ä¿¡æ¯
    """
    return _get_llm_config()

# ç¤ºä¾‹ç”¨æ³•
async def main():
    """æµ‹è¯•LLMæ¨¡å‹æ˜¯å¦å¯ç”¨"""
    print("ğŸ” æµ‹è¯•LLMæ¨¡å‹è¿æ¥çŠ¶æ€...")

    try:
        is_available = await test_llm_connection()
        if is_available:
            print("âœ… LLMæ¨¡å‹å¯ç”¨")
        else:
            print("âŒ LLMæ¨¡å‹ä¸å¯ç”¨")
        return is_available
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def run_tests():
    """è¿è¡ŒLLMæ¨¡å‹å¯ç”¨æ€§æµ‹è¯•"""
    try:
        result = asyncio.run(main())
        return 0 if result else 1
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        return 1


if __name__ == "__main__":
    exit(run_tests())
